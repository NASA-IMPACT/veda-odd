{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ODD Overview","text":"<p>Welcome to the documentation for the Optimized Data Delivery (ODD) team, working on the Visualization, Exploration and Data Analysis (VEDA) and Earth Observation Data on the Cloud (EODC) projects.</p>"},{"location":"#odd-fy26-roadmap","title":"ODD FY26 Roadmap","text":"<p>For a digest of what the team plans to work on this next year, please visit our Fiscal Year (FY) 2026 Roadmap.</p>"},{"location":"adopting/","title":"Adopting the VEDA-ODD Reporting Structure","text":"<p>This guide walks through how to adopt the automated reporting system used by VEDA-ODD for your own team or project. The system tracks open-source contributions across GitHub repositories, generates visualizations, and publishes a documentation site -- all driven by GitHub Issues and Actions.</p>"},{"location":"adopting/#overview","title":"Overview","text":"<p>The reporting structure ties together four pieces:</p> <ol> <li>GitHub Issues define quarterly objectives, contributors, and repositories.</li> <li>Python scripts query the GitHub API for commit data and generate charts and documentation.</li> <li>GitHub Actions run the scripts on a schedule and open a PR with the results.</li> <li>MkDocs publishes the documentation site to GitHub Pages.</li> </ol> <pre><code>GitHub Issues (labeled objectives)\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   weekly cron\n\u2502  generate_config \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 GitHub Actions\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     main.py      \u2502  query commits via GitHub API\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     plot.py      \u2502  create bar chart (matplotlib)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  generate_docs   \u2502  write objectives.md\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25bc\n   Pull Request \u2500\u2500\u25ba merge \u2500\u2500\u25ba MkDocs deploys to GitHub Pages\n</code></pre>"},{"location":"adopting/#prerequisites","title":"Prerequisites","text":"<ul> <li>A GitHub organization or repository where your team works</li> <li>Python 3.11+</li> <li>uv (fast Python package manager)</li> <li>A GitHub fine-grained personal access token (PAT) with public repository read access</li> </ul>"},{"location":"adopting/#step-1-fork-or-copy-the-repository-structure","title":"Step 1: Fork or copy the repository structure","text":"<p>Start by reproducing this directory layout in your own repo:</p> <pre><code>your-repo/\n\u251c\u2500\u2500 .github/\n\u2502   \u251c\u2500\u2500 ISSUE_TEMPLATE/\n\u2502   \u2502   \u2514\u2500\u2500 objective.md          # Issue template for objectives\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 deploy.yml            # MkDocs deployment\n\u2502       \u2514\u2500\u2500 update-reports.yml    # Report generation\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 objectives.md             # Auto-generated (do not edit by hand)\n\u2502   \u2514\u2500\u2500 images/                   # Auto-generated charts\n\u251c\u2500\u2500 reports/\n\u2502   \u251c\u2500\u2500 settings.py               # Team-specific settings (edit this first)\n\u2502   \u251c\u2500\u2500 config.py                 # Objectives, repos, contributors, dates\n\u2502   \u251c\u2500\u2500 main.py                   # Fetch commit data from GitHub\n\u2502   \u251c\u2500\u2500 plot.py                   # Generate visualization\n\u2502   \u251c\u2500\u2500 generate_config.py        # Regenerate config from GitHub issues\n\u2502   \u251c\u2500\u2500 generate_docs.py          # Generate objectives.md\n\u2502   \u251c\u2500\u2500 pyproject.toml            # Python dependencies for reports\n\u2502   \u2514\u2500\u2500 output/                   # CSV output directory\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 pyproject.toml                # Root project dependencies (MkDocs, etc.)\n</code></pre>"},{"location":"adopting/#step-2-set-up-issue-driven-objectives","title":"Step 2: Set up issue-driven objectives","text":"<p>Objectives are GitHub Issues with a specific labeling convention. Each objective issue needs:</p> <ul> <li>A label matching the pattern <code>pi-{YY}.{Q}-objective</code> (e.g., <code>pi-26.2-objective</code> for FY26 Q2).</li> <li>Assignees set to the contributors working on that objective.</li> <li>Repository labels in the format <code>repo:org/repo-name</code> for each tracked repository.</li> </ul>"},{"location":"adopting/#create-an-issue-template","title":"Create an issue template","text":"<p>Add <code>.github/ISSUE_TEMPLATE/objective.md</code> so team members can propose objectives with a consistent structure:</p> <pre><code>---\nname: Propose objective\nabout: Objective for the team\ntitle: 'PI 26.2 Objective X: ...'\nlabels: pi-26.2-objective\nassignees: ''\n---\n\n### Motivation\n\n### Description\n\n### Acceptance Criteria\n\n- [ ] ...\n\n### Sub-tasks\n\n- [ ] ...\n</code></pre> <p>Update the <code>title</code> and <code>labels</code> fields each quarter to match the current PI.</p>"},{"location":"adopting/#label-conventions","title":"Label conventions","text":"<p>Create these labels in your repository:</p> Label Purpose <code>pi-{YY}.{Q}-objective</code> Tags an issue as a quarterly objective <code>repo:org/repo-name</code> Maps a repository to an objective <p>The <code>generate_config.py</code> script searches for issues using the <code>pi-*-objective</code> label pattern, so sticking to this convention is required.</p>"},{"location":"adopting/#step-3-configure-team-settings","title":"Step 3: Configure team settings","text":"<p>Open <code>reports/settings.py</code> and update the values for your team:</p> <pre><code>GITHUB_ORG = \"your-org\"                    # GitHub org that owns the repo\nGITHUB_REPO = \"your-repo\"                  # repo where objective issues live\nTEAM_NAME = \"Your Team\"                    # short name for chart titles\nTEAM_DISPLAY_NAME = \"Your Full Team Name\"  # used in chart annotations\nSITE_URL = \"your-org.github.io/your-repo\"  # GitHub Pages URL (no https://)\nTOKEN_ENV_VAR = \"GH_YOUR_TEAM_PAT\"        # env var name for the GitHub PAT\n</code></pre> <p>This is the only Python file you need to edit to adopt the reporting system. All other scripts import their team-specific values from here. The derived values (<code>REPO_FULL_NAME</code>, <code>REPO_URL</code>, <code>OBJECTIVES_PAGE_URL</code>) are computed automatically.</p> <p>You will also need to update the secret name in <code>.github/workflows/update-reports.yml</code> to match your <code>TOKEN_ENV_VAR</code> value, since the workflow YAML cannot import from Python.</p>"},{"location":"adopting/#step-4-configure-the-reporting-scripts","title":"Step 4: Configure the reporting scripts","text":""},{"location":"adopting/#define-pi-date-ranges","title":"Define PI date ranges","text":"<p>In <code>reports/config.py</code>, define the date ranges for each Program Increment (PI). VEDA-ODD uses a fiscal-quarter calendar:</p> <pre><code>PI_DATES = {\n    \"pi-26.1\": (\"20251018\", \"20260117\"),\n    \"pi-26.2\": (\"20260118\", \"20260425\"),\n    # Add your own quarters here\n}\n</code></pre> <p>Dates are in <code>YYYYMMDD</code> format. The <code>get_current_pi()</code> helper determines which PI is active based on today's date.</p>"},{"location":"adopting/#define-objectives","title":"Define objectives","text":"<p>The <code>OBJECTIVES</code> dictionary in <code>config.py</code> is the source of truth for what gets tracked. Each entry maps a PI to a list of objectives:</p> <pre><code>OBJECTIVES = {\n    \"pi-26.2\": [\n        {\n            \"issue_number\": 304,\n            \"title\": \"Objective description\",\n            \"state\": \"open\",\n            \"contributors\": [\n                (\"Display Name\", \"github_username\"),\n            ],\n            \"repos\": [\n                (\"org-name\", \"repo-name\"),\n            ],\n        },\n        # more objectives ...\n    ],\n}\n</code></pre> <p>You can maintain this manually or auto-generate it with <code>generate_config.py</code> (see below).</p>"},{"location":"adopting/#auto-generate-config-from-issues","title":"Auto-generate config from issues","text":"<p>Run <code>generate_config.py</code> to pull objectives, contributors, and repo mappings from your GitHub issues:</p> <pre><code>cd reports\nexport GH_YOUR_TEAM_PAT=ghp_your_token_here  # must match TOKEN_ENV_VAR in settings.py\nuv run generate_config.py\n</code></pre> <p>This produces an <code>objectives_config.py</code> file. Review it, then copy the relevant sections into <code>config.py</code>. The script extracts:</p> <ul> <li>Objective metadata from issue titles</li> <li>Contributors from issue assignees</li> <li>Repository mappings from <code>repo:org/repo-name</code> labels</li> </ul>"},{"location":"adopting/#install-python-dependencies","title":"Install Python dependencies","text":"<p>The reports scripts have their own <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"reports\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\n    \"matplotlib&gt;=3.10.3\",\n    \"pandas&gt;=2.3.0\",\n    \"pygithub&gt;=2.6.1\",\n]\n</code></pre> <p>Run <code>uv sync</code> in the <code>reports/</code> directory to install them.</p>"},{"location":"adopting/#step-5-run-the-reporting-pipeline-locally","title":"Step 5: Run the reporting pipeline locally","text":"<p>The pipeline has four stages that must run in order:</p> <pre><code>cd reports\n\n# 1. (Optional) Regenerate config from GitHub issues\nuv run generate_config.py\n\n# 2. Fetch commit data for the current PI\nuv run main.py\n\n# 3. Generate the bar chart\nuv run plot.py\n\n# 4. Generate the objectives documentation page\nuv run generate_docs.py\n</code></pre> <p>After running, you will have:</p> Output Location Commit CSV <code>reports/output/pi-{PI}.csv</code> Bar chart PNG <code>docs/images/pi-{PI}.png</code> Objectives page <code>docs/objectives.md</code>"},{"location":"adopting/#environment-variables","title":"Environment variables","text":"Variable Required Description Value of <code>TOKEN_ENV_VAR</code> in <code>settings.py</code> Yes Fine-grained PAT with public repo read access <code>GITHUB_TOKEN</code> In CI only Automatically provided by GitHub Actions <p>To create a PAT: go to GitHub token settings, select Public Repositories, and generate the token.</p>"},{"location":"adopting/#step-6-set-up-mkdocs","title":"Step 6: Set up MkDocs","text":""},{"location":"adopting/#root-pyprojecttoml","title":"Root <code>pyproject.toml</code>","text":"<p>The documentation site dependencies live in the root project file:</p> <pre><code>[dependency-groups]\ndev = [\n    \"mkdocs&gt;=1.6.1\",\n    \"mkdocs-material[imaging]&gt;=9.6.3\",\n    \"mike&gt;=2.1.3\",\n]\n</code></pre>"},{"location":"adopting/#mkdocsyml","title":"<code>mkdocs.yml</code>","text":"<p>Configure your site. At a minimum, include the objectives page in your nav:</p> <pre><code>site_name: Your Team Reports\nnav:\n  - \"index.md\"\n  - PI Objectives: \"objectives.md\"\n\ntheme:\n  name: material\n\nplugins:\n  - search\n</code></pre>"},{"location":"adopting/#preview-locally","title":"Preview locally","text":"<pre><code>uv run mkdocs serve\n</code></pre> <p>This starts a local dev server at <code>http://127.0.0.1:8000</code> with live reloading.</p>"},{"location":"adopting/#step-7-automate-with-github-actions","title":"Step 7: Automate with GitHub Actions","text":""},{"location":"adopting/#report-generation-workflow","title":"Report generation workflow","text":"<p>Create <code>.github/workflows/update-reports.yml</code>:</p> <pre><code>name: Update Reports\n\non:\n  schedule:\n    # Run every Monday at 9 AM ET (14:00 UTC)\n    - cron: '0 14 * * 1'\n  push:\n    branches:\n      - main\n    paths:\n      - 'reports/*.py'\n      - 'reports/pyproject.toml'\n  workflow_dispatch:\n\njobs:\n  update-reports:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n      pull-requests: write\n\n    steps:\n      - uses: actions/checkout@v5\n\n      - uses: astral-sh/setup-uv@v7\n        with:\n          version: \"0.9.*\"\n          enable-cache: true\n\n      - name: Generate config data\n        working-directory: reports\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          GH_ODD_PAT: ${{ secrets.GH_ODD_PAT }}  # Rename to match TOKEN_ENV_VAR\n        run: uv run generate_config.py\n\n      - name: Generate commit data\n        working-directory: reports\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          GH_ODD_PAT: ${{ secrets.GH_ODD_PAT }}  # Rename to match TOKEN_ENV_VAR\n        run: uv run main.py\n\n      - name: Generate plot\n        working-directory: reports\n        run: uv run plot.py\n\n      - name: Generate docs page\n        working-directory: reports\n        run: uv run generate_docs.py\n\n      - name: Create Pull Request\n        uses: peter-evans/create-pull-request@v7\n        with:\n          commit-message: \"Update reports\"\n          title: \"Update reports\"\n          body: |\n            Automated update of commit reports and visualization.\n          branch: update-reports\n          add-paths: |\n            reports/output/\n            docs/images/\n            docs/objectives.md\n</code></pre>"},{"location":"adopting/#deployment-workflow","title":"Deployment workflow","text":"<p>Create <code>.github/workflows/deploy.yml</code>:</p> <pre><code>name: Deploy docs\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v5\n\n      - uses: astral-sh/setup-uv@v7\n        with:\n          version: \"0.9.*\"\n          enable-cache: true\n\n      - run: uv run mkdocs gh-deploy --force --strict\n</code></pre>"},{"location":"adopting/#repository-secrets","title":"Repository secrets","text":"<p>Add the following secret in your repository settings (Settings &gt; Secrets and variables &gt; Actions):</p> Secret Value Secret matching <code>TOKEN_ENV_VAR</code> in <code>settings.py</code> Your fine-grained personal access token (default: <code>GH_ODD_PAT</code>) <p><code>GITHUB_TOKEN</code> is provided automatically by GitHub Actions.</p>"},{"location":"adopting/#step-8-quarterly-maintenance","title":"Step 8: Quarterly maintenance","text":"<p>At the start of each quarter:</p> <ol> <li>Update the issue template -- change the label and title prefix to the new PI (e.g., <code>pi-26.3-objective</code>).</li> <li>Create objective issues -- use the template to propose and finalize objectives. Assign contributors and add <code>repo:org/repo-name</code> labels.</li> <li>Add the new PI date range to <code>PI_DATES</code> in <code>config.py</code>.</li> <li>Run <code>generate_config.py</code> or wait for the weekly automation to pick up the new issues.</li> </ol> <p>The weekly cron job handles everything else: fetching commit data, generating the chart, updating the docs page, and opening a PR for review.</p>"},{"location":"adopting/#customization","title":"Customization","text":""},{"location":"adopting/#team-settings","title":"Team settings","text":"<p>All team-specific values (org name, repo name, team display name, PAT variable name, site URL) are centralized in <code>reports/settings.py</code>. This is the first file to edit when adopting the system for a new team. The workflow YAML file (<code>.github/workflows/update-reports.yml</code>) must also be updated to use your team's secret name for the GitHub PAT.</p>"},{"location":"adopting/#adjusting-the-chart","title":"Adjusting the chart","text":"<p><code>plot.py</code> generates a horizontal bar chart color-coded by objective. You can customize:</p> <ul> <li>Colors: Edit the <code>COLORS</code> list (10-color palette that cycles).</li> <li>Figure size: Change the <code>figsize</code> parameter (default: 16x10 inches).</li> <li>DPI: Adjust output resolution (default: 150).</li> <li>Caveats text: Update the annotation at the bottom of the chart.</li> </ul> <p>Repositories that appear in multiple objectives get split bars showing proportional contribution.</p>"},{"location":"adopting/#tracking-different-metrics","title":"Tracking different metrics","text":"<p><code>main.py</code> collects commit-level data including SHA, message, author, and total changes (additions + deletions). The output CSV can be extended or replaced with other metrics by modifying the <code>get_commits_for_repo_user</code> function.</p>"},{"location":"adopting/#changing-the-schedule","title":"Changing the schedule","text":"<p>Edit the <code>cron</code> expression in <code>update-reports.yml</code>. For example, to run daily at noon UTC:</p> <pre><code>schedule:\n  - cron: '0 12 * * *'\n</code></pre>"},{"location":"adopting/#troubleshooting","title":"Troubleshooting","text":"Problem Solution <code>generate_config.py</code> finds no issues Verify your issues have labels matching <code>pi-*-objective</code> Rate limiting errors in <code>main.py</code> Ensure your PAT env var (see <code>TOKEN_ENV_VAR</code> in <code>settings.py</code>) is set. The script uses 10 parallel workers; reduce this if needed Chart is empty Check that <code>reports/output/pi-{PI}.csv</code> has data. Verify date ranges in <code>PI_DATES</code> PR not created by Actions Confirm the workflow has <code>contents: write</code> and <code>pull-requests: write</code> permissions MkDocs build fails Run <code>uv run mkdocs build --strict</code> locally to see errors"},{"location":"fy26-roadmap/","title":"ODD Fiscal Year (FY) 2026 Roadmap","text":"<p>If you are interested in a better understanding of the ODD service roadmap, and what datasets will be supported when, this document is for you.</p> <p>This document provides a roadmap for the VEDA Optimized Data Delivery Team (ODD), broken into 4 categories: 1. Services for granules in CMR 2. Services for datacubes 3. Services non-datacube 4. Foundational Work</p> <p>It is important to note that this roadmap is a reflection of the team's current plans, written as of November 2025. These are likely to evolve over time. We intend to update the roadmap quarterly.</p> <p>For a higher-level vision, see also: Optimized Data Delivery Roadmap for NASA - July 2025.</p>"},{"location":"fy26-roadmap/#legend","title":"Legend","text":"<ul> <li>\u2705 Complete - Already delivered</li> <li>\ud83d\udea7 In Progress - Active development</li> <li>\ud83d\udd04 Ongoing - Ongoing work</li> <li>\ud83d\udcc5 Planned - Scheduled for specific quarter</li> <li>\ud83d\udd2e Future - Planned for future timeline</li> </ul>"},{"location":"fy26-roadmap/#roadmap-for-service-category-1-services-for-cmr-granules","title":"Roadmap for Service Category 1: Services for CMR Granules","text":""},{"location":"fy26-roadmap/#access","title":"Access","text":"<p>N/A</p>"},{"location":"fy26-roadmap/#visualization","title":"Visualization","text":"<ul> <li>\u2705 Complete titiler-cmr /tiles API + VEDA UI integration</li> </ul>"},{"location":"fy26-roadmap/#timeseries","title":"Timeseries","text":"<ul> <li>\u2705 Complete titiler-cmr /timeseries/statistics API + VEDA UI integration</li> </ul>"},{"location":"fy26-roadmap/#additional-features","title":"Additional Features","text":"<ul> <li>\ud83d\udea7 26.1 Release /compatibility endpoint</li> <li>\ud83d\udcc5 26.2+ Develop support for more datasets, informed by compatibility testing in 26.1.</li> </ul>"},{"location":"fy26-roadmap/#dataset-support","title":"Dataset Support","text":"<ul> <li>\u2705 Complete Demonstrated with GPM IMERG, TROPESS O3 and MiCASA</li> <li>\ud83d\udea7 26.1 Compile a list of compatible datasets</li> <li>\ud83d\udea7 26.1 Develop support for EDL-based credential access, as an aternative to requester-pays and role-based access. To support NISAR (ASF) and GEDI L4B (ORNL DAAC) specifically.</li> <li>\ud83d\udcc5 26.2+ Test integration of new datasets as requester-pays is enabled for more buckets.</li> </ul>"},{"location":"fy26-roadmap/#performance-operations","title":"Performance + Operations","text":"<ul> <li>\ud83d\udea7 26.1 Deploy monitoring + performance evaluation via service tracing (OpenTelemetry)</li> <li>\ud83d\udcc5 26.1 MCP Production deployment</li> <li>\ud83d\udcc5 26.2 Consolidated benchmarking utilities for advising users on zoom levels, AOIs and temporal parameters on a per-dataset basis</li> </ul>"},{"location":"fy26-roadmap/#ecosystem-development","title":"Ecosystem Development","text":"<ul> <li>\ud83d\udcc5 26.2 Share compatible dataset list with NASA product teams for potential integration (i.e. Worldview)</li> <li>\ud83d\udcc5 26.2+ Continued documentation to support self-service use of titiler-cmr.</li> </ul>"},{"location":"fy26-roadmap/#roadmap-for-service-category-2-services-for-datacubes","title":"Roadmap for Service Category 2: Services for Datacubes","text":""},{"location":"fy26-roadmap/#access_1","title":"Access","text":"<ul> <li>\u2705 Complete Lazy loading/intelligent subsetting/intelligent access for varied data formats (GRIB, COG, NetCDF-4, HDF5 via VirtualiZarr)</li> <li>\ud83d\udcc5 26.1 Support adoption of Virtual Zarr through library maintenance, improved documentation, and user support</li> <li>\ud83d\udcc5 26.2 Support for arbitrary chunk-grids (variable chunking)</li> <li>\ud83d\udcc5 26.2 Explore virtualization methods for alternate grid structures (i.e., healpix, cubegrid)</li> </ul>"},{"location":"fy26-roadmap/#visualization_1","title":"Visualization","text":"<ul> <li>\ud83d\udcc5 26.1 Virtual container (Icechunk) integration in titiler-multidim to support /tiles endpoints</li> <li>\ud83d\udcc5 26.1 Identify additional I/O parameters to allow for per-dataset optimizations</li> <li>\ud83d\udcc5 26.1 Test VEDA UI integration of /tiles for a virtual dataset (e.g. NLDAS)</li> <li>\ud83d\udcc5 26.2 Additional performance improvements (e.g. obstore integration)</li> </ul>"},{"location":"fy26-roadmap/#timeseries_1","title":"Timeseries","text":"<ul> <li>\ud83d\udcc5 26.1 Design the timeseries/statistics endpoint to support datacubes (i.e. could be an asynchronous API outside the titiler ecosystem)</li> <li>\ud83d\udcc5 26.2 Develop the timeseries/statistics endpoint</li> <li>\ud83d\udcc5 26.2 Integrate the timeseries/statistics endpoint into VEDA UI</li> </ul>"},{"location":"fy26-roadmap/#datasets","title":"Datasets","text":"<ul> <li>\u2705 Complete Prototyped virtual (Icechunk) stores for NLDAS, RASI, HRRR, MUR SST</li> <li>\ud83d\udcc5 26.1 Demonstrate publication and tiling of NLDAS virtual store (\ud83d\udca7 Water Insight)</li> <li>\ud83d\udcc5 26.1 Architecture + documentation for generalizing STAC publication and VEDA UI /tiles integration</li> <li>\ud83d\udcc5 26.2 HydroGlobe 5km and 10km virtual stores (\ud83d\udca7 Water Insight)</li> <li>\ud83d\udcc5 26.2 CarbonTracker-CH\u2084, EPA Gridded CH\u2084 Emissions Inventory virtual stores (\ud83c\udfed GHGCenter)</li> <li>\ud83d\udcc5 26.3 Documentation for STAC publication and VEDA UI /timeseries/statistics integration</li> <li>\ud83d\udcc5 26.3 CarbonTracker-CH\u2084, EPA Gridded CH\u2084 Emissions Inventory tiles and timeseries integrations (\ud83c\udfed GHGCenter)</li> <li>\ud83d\udcc5 26.3 TROPESS NOx, TROPESS O3, JPL MOMO Chem, GEOS CF virtual stores, tiles and timeseries integrations (\ud83d\udca8 Air Quality)</li> </ul>"},{"location":"fy26-roadmap/#operations","title":"Operations","text":"<ul> <li>\ud83d\udcc5 26.2 Monitoring + Performance evaluation via service tracing (OpenTelemetry)</li> <li>\ud83d\udcc5 26.3 MCP deployment</li> <li>\ud83d\udcc5 26.2 Consolidated benchmarking utilities for advising users on zoom levels, AOIs and temporal parameters on a per-dataset basis</li> </ul>"},{"location":"fy26-roadmap/#ecosystem-development_1","title":"Ecosystem Development","text":"<ul> <li>\ud83d\udcc5 26.1 Create template data ingestion pipeline for virtualizing datasets</li> <li>\ud83d\udcc5 26.3+ Moving towards self-service integration</li> </ul>"},{"location":"fy26-roadmap/#roadmap-for-service-category-3-services-for-non-datacubes","title":"Roadmap for Service Category 3: Services for Non-Datacubes","text":""},{"location":"fy26-roadmap/#access_2","title":"Access","text":"<ul> <li>\ud83d\udea7 26.1-26.3 Prototyping creating a query engine using a Zarr provider for data fusion</li> </ul>"},{"location":"fy26-roadmap/#visualization_2","title":"Visualization","text":"<ul> <li>\ud83d\udd2e 26.4 or FY 27 Tiling endpoints in near-term, direct client approaches in long-term</li> </ul>"},{"location":"fy26-roadmap/#timeseries_2","title":"Timeseries","text":"<ul> <li>\ud83d\udd2e 26.4 or FY 27 Timeseries API</li> </ul>"},{"location":"fy26-roadmap/#datasets_1","title":"Datasets","text":"<ul> <li>\ud83d\udcc5 26.1 Prototype HLS store</li> <li>\ud83d\udcc5 26.3+ Prototype NISAR and/or Opera stores</li> </ul>"},{"location":"fy26-roadmap/#operations_1","title":"Operations","text":"<ul> <li>\ud83d\udd2e 26.4 or FY 27 Operational deployment + documentation</li> <li>\ud83d\udd2e 26.4 or FY 27 Consolidated benchmarking utilities for advising users on zoom levels, AOIs and temporal parameters on a per-dataset basis</li> </ul>"},{"location":"fy26-roadmap/#ecosystem-development_2","title":"Ecosystem Development","text":"<ul> <li>\ud83d\udd2e 26.4 or FY 27 Develop ecosystem, moving towards self-service adoption within VEDA and broader community</li> </ul>"},{"location":"fy26-roadmap/#roadmap-for-service-category-4-foundational-work-including-technical-debt","title":"Roadmap for Service Category 4: Foundational Work (including Technical Debt)","text":"<ul> <li>\ud83d\udd04 26.1+ Establish areas for consolidation in the TiTiler ecosystem. Similar features across applications should rely on shared upstream libraries. The ODD team continuously identifying similar features and proactively DRY up codebases.</li> </ul>"},{"location":"objectives/","title":"Quarterly Objectives","text":"<p>This page tracks quarterly objectives and their related repositories across Program Increments (PIs).</p>"},{"location":"objectives/#current-pi-262","title":"Current PI: 26.2","text":"# Objective Contributors Repos #304 \ud83e\udd16Create virtual Zarr stores for high priority dataset(s) tylanderson virtualizarr-data-pipelines #305 \ud83d\udee0\ufe0f TiTiler-CMR maintenance + performance chuckwondo, hrodmn titiler, titiler-cmr, python_cmr, earthaccess, rio-tiler #306 \ud83e\udd17 Support community adoption of the technologies incubated b... chuckwondo, maxrjones, abarciauskas-bgse geozarr-spec, zarr-python, multiscales, geo-proj, spatial, datacube-guide, geozarr-examples, warp-resample-profiling, pangeo.io, pangeo-docker-images, earthdata-cloud-cookbook, virtualzarr.cloud #307 \ud83d\udcc8 Finalize Design for VirtualiZarr Time Series API [Stretch:... hrodmn titiler, titiler-lambda-layer, titiler-md-demo #308 \ud83e\udd2a Expand virtualization support for quirky datasets maxrjones virtualizarr, zarr-python, obspec-utils, virtual-tiff, hrrr-parser, virtualizarr-data-pipelines, nisar-manifest-explorer, mur-manifest-explorer #309 \ud83d\udef0\ufe0f Propose unified strategy for virtualization of orbital sw... sharkinsspatial, maxrjones, hrodmn virtualizarr, obspec-utils, virtual-tiff, obstore, obspec, zarr-datafusion-search, geoarrow-rs, async-tiff, arrow-zarr #319 \ud83c\udf10 Client-side raster rendering (deck.gl-raster) kylebarron - PI 26.1 (5 objectives, 3 closed) # Objective State Contributors #244 \ud83d\uddfa\ufe0f Add dynamic tiling and timeseries support for V... open jbusecke, hrodmn #245 \ud83c\udf0d Add dynamic tiling and timeseries support for da... open abarciauskas-bgse, hrodmn #246 \ud83e\udd16 Support virtualization of additional data produc... closed sharkinsspatial, maxrjones, jbusecke #247 \ud83d\udef0 Explore scalable, cloud native approaches for se... closed sharkinsspatial, kylebarron #248 \ud83e\udd17 Support community adoption of the technologies i... closed sharkinsspatial, chuckwondo, maxrjones, abarciauskas-bgse PI 25.4 (8 objectives, 7 closed) # Objective State Contributors #121 Visualize Web-Optimized Zarr (WOZ) in VEDA (previe... closed maxrjones #122 Research, develop and document methods for Zarr an... closed maxrjones, kylebarron #197 \ud83c\udfac TiTiler-CMR is production ready open abarciauskas-bgse, hrodmn #198 \ud83d\ude80 Dataset support for VEDA instances closed maxrjones, jbusecke #203 \ud83d\uddfa\ufe0fResearch, develop and document methods for Zarr ... closed maxrjones #204 \ud83d\udee0\ufe0f Zarr Development closed d-v-b, maxrjones #205 \ud83e\udd17 Community engagement closed sharkinsspatial, chuckwondo, maxrjones, abarciauskas-bgse #206 \ud83d\udce6 Obstore outreach closed chuckwondo, kylebarron PI 25.3 (6 objectives, 6 closed) # Objective State Contributors #118 Support CMR Modernization closed sharkinsspatial, kylebarron #119 Continue to Build Out the VirtualiZarr Ecosystem closed sharkinsspatial, maxrjones #124 Publish Cloud-Optimized Datasets closed chuckwondo, abarciauskas-bgse #126 Support TiTiler-CMR Adoption closed sharkinsspatial, hrodmn #127 Community Involvement closed maxrjones, abarciauskas-bgse, hrodmn #165 Foundational Zarr-Python and Xarray Contributions closed d-v-b, maxrjones PI 25.2 (8 objectives, 8 closed) # Objective State Contributors #31 Increase data format support in VirtualiZarr closed chuckwondo, maxrjones #34 Visualize OCO-3 Datasets in VEDA closed abarciauskas-bgse #35 Deliver Virtual Zarr Stores for NASA Datasets Usin... closed abarciauskas-bgse #36 Support for Modernizing VirtualiZarr to use zarr-p... closed sharkinsspatial, abarciauskas-bgse #37 Support CMR Modernization closed sharkinsspatial, kylebarron #40 Upgrade titiler and titiler-xarray to zarr-Python ... closed maxrjones #41 Draft Web-Optimized Zarr (WOZ) Standard closed maxrjones #76 Demonstrate how to tile HLS using titiler-cmr closed hrodmn"},{"location":"objectives/#visualization","title":"Visualization","text":"<p>The commits per repository chart uses color-coding to show which objective each repo contributes to. Repos that contribute to multiple objectives are shown with split bars.</p> <p></p>"},{"location":"objectives/#configuration","title":"Configuration","text":"<p>Objectives are configured in <code>reports/config.py</code>.</p> <p>To regenerate this page from config:</p> <pre><code>cd reports\nuv run generate_docs.py\n</code></pre> <p>See FY26 Roadmap for the broader context of these objectives.</p>"},{"location":"products/","title":"Products from the VEDA ODD team","text":""},{"location":"products/#presentations","title":"Presentations","text":"<p>This is a non-exhaustive list of presentations by the VEDA ODD team:</p> <ul> <li>2025-12-10 - The GeoZarr Solution</li> <li>2025-10-18 - OGC NetCDF SWG Presentation: The GeoZarr challenge</li> <li>2025-10-16 - The GeoZarr Challenge</li> <li>2025-07-22 - VirtualiZarr 2.0 - Create virtual Zarr stores for cloud-friendly access to archival data, using familiar xarray syntax</li> <li>2025-05-02 - A community oriented approach to enabling open science with Earth science data at scale</li> </ul>"},{"location":"products/#documentation","title":"Documentation","text":"<p>This is a non-exhaustive list of guides, reports, and other materials built by the VEDA ODD team:</p> <ul> <li>Zarr Visualization Report</li> <li>Data in the Cloud 101 Module for Open Source Geospatial Workflows in the Cloud Workshop</li> <li>Zarr DataFusion Search Examples</li> </ul>"},{"location":"products/#software","title":"Software","text":"<p>This is a non-exhaustive list of libraries and tools built by the VEDA ODD team:</p> <ul> <li>Virtual-TIFF</li> <li>Zarr-Datafusion-Search</li> </ul>"},{"location":"products/#contributions","title":"Contributions","text":"<p>ODD also regularly contributes to these guides, tools, and libraries, which were built outside the VEDA/EODC pojects:</p> <ul> <li>Cloud Native Geospatial Data Formats Guide</li> <li>Datacube-guide</li> <li>VirtualiZarr</li> <li>Zarr</li> <li>GeoZarr</li> </ul>"},{"location":"products/#similar-external-resources","title":"Similar external resources","text":"<p>If you are interested in the tools that VEDA ODD has contributed to, you may also enjoy these completely externally built materials:</p> <ul> <li>2025 ICESAT-2 hackweek module on cloud computing</li> </ul>"},{"location":"products/#acknowledgements","title":"Acknowledgements","text":"<p>VEDA ODD is fortunate to be part of the Data Systems Evolution team at NASA. The Data Systems Evolution team at NASA Marshall Space Flight Center's Office of Data Science and Informatics (ODSI) enables scientific exploration and discovery through innovative data visualization techniques and analysis capabilities that lower the barrier to entry for cloud-hosted data.</p>"},{"location":"tech-tips/","title":"Tech tips and tricks","text":"<p>This is a casual page for team members to document helpful tips and tricks.</p>"},{"location":"tech-tips/#compiling-recent-github-activity-for-reporting","title":"Compiling recent GitHub activity for reporting","text":"<p>Here's a bash script that can be modified to get pull request and issue activity for one or more users over a given time frame:</p> <pre><code>#!/bin/bash\n\nAUTHORS=(maxrjones)\nDATE_RANGE=\"2026-02-02..2026-02-08\"\nOUTPUT=\"output.md\"\n\nPR_JQ='\n  group_by(.repository.nameWithOwner) |\n  .[] |\n  \"\\n### \\(.[0].repository.nameWithOwner) (\\(length) PRs)\\n\" + (\n    .[] | \"#\\(.number) [\\(.state)] \\(.author.login): \\(.title)\\n  Created: \\(.createdAt | split(\"T\")[0]) | Closed: \\(if .closedAt then (.closedAt | split(\"T\")[0]) else \"\u2014\" end)\\n  \\(.url)\"\n  )\n'\n\nISSUE_JQ='\n  group_by(.repository.nameWithOwner) |\n  .[] |\n  \"\\n### \\(.[0].repository.nameWithOwner) (\\(length) issues)\\n\" + (\n    .[] | \"#\\(.number) [\\(.state)] \\(.author.login): \\(.title)\\n  Created: \\(.createdAt | split(\"T\")[0]) | Closed: \\(if .closedAt then (.closedAt | split(\"T\")[0]) else \"\u2014\" end)\\n  \\(.url)\"\n  )\n'\n\n&gt; \"$OUTPUT\"\n\nfor author in \"${AUTHORS[@]}\"; do\n  echo \"# Contributions: $author\" &gt;&gt; \"$OUTPUT\"\n  echo \"**Period:** $DATE_RANGE\" &gt;&gt; \"$OUTPUT\"\n\n  # --- Authored PRs ---\n  echo -e \"\\n## Authored PRs\" &gt;&gt; \"$OUTPUT\"\n  gh search prs \\\n    --limit 1000 \\\n    --author=\"$author\" \\\n    --updated=\"$DATE_RANGE\" \\\n    --json number,title,state,createdAt,closedAt,url,repository,author \\\n  | jq -r \"$PR_JQ\" &gt;&gt; \"$OUTPUT\"\n\n  # --- PR Reviews (PRs reviewed by the author, excluding self-authored) ---\n  echo -e \"\\n## PR Reviews\" &gt;&gt; \"$OUTPUT\"\n  gh search prs \\\n    --limit 1000 \\\n    --reviewed-by=\"$author\" \\\n    --updated=\"$DATE_RANGE\" \\\n    --json number,title,state,url,repository,author \\\n  | jq -r --arg self \"$author\" '\n    [ .[] | select(.author.login != $self) ] |\n    if length == 0 then \"No reviews found\"\n    else\n      group_by(.repository.nameWithOwner) |\n      .[] |\n      \"\\n### \\(.[0].repository.nameWithOwner) (\\(length) PRs reviewed)\\n\" + (\n        .[] | \"#\\(.number) [\\(.state)] by \\(.author.login): \\(.title)\\n  \\(.url)\"\n      )\n    end\n  ' &gt;&gt; \"$OUTPUT\"\n\n  # --- Issue Engagement (authored) ---\n  echo -e \"\\n## Issues Authored\" &gt;&gt; \"$OUTPUT\"\n  gh search issues \\\n    --limit 1000 \\\n    --author=\"$author\" \\\n    --updated=\"$DATE_RANGE\" \\\n    --json number,title,state,createdAt,closedAt,url,repository,author \\\n  | jq -r \"$ISSUE_JQ\" &gt;&gt; \"$OUTPUT\"\n\n  # --- Issue Engagement (involved \u2014 commented, mentioned, assigned) ---\n  echo -e \"\\n## Issues Engaged (commented/assigned/mentioned)\" &gt;&gt; \"$OUTPUT\"\n  gh search issues \\\n    --limit 1000 \\\n    --involves=\"$author\" \\\n    --updated=\"$DATE_RANGE\" \\\n    --json number,title,state,createdAt,closedAt,url,repository,author \\\n  | jq -r --arg self \"$author\" '\n    [ .[] | select(.author.login != $self) ] |\n    if length == 0 then \"No engagement found\"\n    else\n      group_by(.repository.nameWithOwner) |\n      .[] |\n      \"\\n### \\(.[0].repository.nameWithOwner) (\\(length) issues)\\n\" + (\n        .[] | \"#\\(.number) [\\(.state)] by \\(.author.login): \\(.title)\\n  \\(.url)\"\n      )\n    end\n  ' &gt;&gt; \"$OUTPUT\"\n\ndone\n</code></pre>"}]}